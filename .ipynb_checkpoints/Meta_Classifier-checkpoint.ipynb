{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final version of the classifiers and meta classifier script\n",
    "\n",
    "\n",
    "### Parameter settings\n",
    "\n",
    "First, parameters are set. The parameters are obtained from a crossvalidation performed for each seperate classifier. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Attributes from dataset that we use for classification\n",
    "\n",
    "nAttributes = ['Age', 'popularity resquer id', 'Breed1', 'img_pixels', 'PhotoAmt', 'Sterilized', 'description length',\n",
    "               'img_ave_contrast', 'Breed2', 'Quantity', 'Gender', 'img_metadata_sentiment2', 'beaut', 'MaturitySize',\n",
    "               'State',  'Color3', 'vaccin', 'abandon', 'Vaccinated', 'Fee', 'indoor', 'cute', 'great']\n",
    "\n",
    "\n",
    "max_depth = 8  #max depth of decision tree\n",
    "n_estimators = 10  # number of trees in random forest\n",
    "tol = 0.01  #tolerance in (gradient/line) search in Support Vector Machine classifier, Logistic Regression\n",
    "nn = 15  # number of neighbors for K-Nearest Neighbor classifier\n",
    "xgb_params = {  #parameters for XGBoost\n",
    "    'eval_metric': 'rmse',\n",
    "    'seed': 1337,\n",
    "    'verbosity': 0,\n",
    "}   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Some additional pre-processing\n",
    "\n",
    "Data is loaded and relevant attributes are obtained. A gaussian transform and normalisation transform is applied to the appropriate variables. Dataset is also converted with dummy variables for categorical attributes so that Logistic Regression can be performed. \n",
    "\n",
    "### Meta train/test set\n",
    "\n",
    "The train data is divided into a meta train and meta test set, to train and test the meta classifier respectively. 10% of the train data is used for meta testing, which helps select the best model for meta classifying.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lisa\\Anaconda3\\lib\\site-packages\\scipy\\stats\\morestats.py:902: RuntimeWarning: divide by zero encountered in log\n",
      "  llf -= N / 2.0 * np.log(np.sum((y - y_mean)**2. / N, axis=0))\n",
      "C:\\Users\\Lisa\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\data.py:625: DataConversionWarning: Data with input dtype int64, float64 were all converted to float64 by StandardScaler.\n",
      "  return self.partial_fit(X, y)\n",
      "C:\\Users\\Lisa\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:53: DataConversionWarning: Data with input dtype int64, float64 were all converted to float64 by StandardScaler.\n",
      "C:\\Users\\Lisa\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:54: DataConversionWarning: Data with input dtype int64, float64 were all converted to float64 by StandardScaler.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn import tree\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "import xgboost as xgb\n",
    "\n",
    "from sklearn import model_selection\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import PowerTransformer\n",
    "from sklearn import preprocessing\n",
    "\n",
    "\n",
    "\n",
    "data = pd.read_csv('Data/preprocessedTrain3.csv') #import data\n",
    "X = data.loc[:, data.columns != 'AdoptionSpeed'] #create X without labels\n",
    "X = X.fillna(0)\n",
    "X = X.drop('Description',axis=1) #drop non numerical values\n",
    "X = X.drop('PetID',axis=1) #\n",
    "X = X.drop('RescuerID',axis=1)\n",
    "X = X.drop('Unnamed: 0',axis=1)\n",
    "X = X.drop('Unnamed: 0.1',axis=1)\n",
    "X = X.drop('img_metadata_label',axis=1)\n",
    "X = X[nAttributes]\n",
    "y = data['AdoptionSpeed'] #label vector\n",
    "\n",
    "test = pd.read_csv('Data/preprocessedtest3.csv')\n",
    "\n",
    "\n",
    "X_test = test.drop('Description',axis=1) #drop non numerical values\n",
    "X_test = X_test.fillna(0)\n",
    "id = X_test['PetID']\n",
    "X_test = X_test.drop('PetID',axis=1) #\n",
    "X_test = X_test.drop('RescuerID',axis=1)\n",
    "X_test = X_test.drop('Unnamed: 0',axis=1)\n",
    "X_test = X_test.drop('Unnamed: 0.1',axis=1)\n",
    "\n",
    "X_test = X_test.drop('img_metadata_label',axis=1)\n",
    "X_test = X_test[nAttributes]\n",
    "\n",
    "\n",
    "non_zer0 = np.mean(X==0)==0\n",
    "zero = non_zer0[non_zer0.values==False].index\n",
    "non_zer0 = non_zer0[non_zer0.values==True].index\n",
    "\n",
    "scaler = preprocessing.PowerTransformer(method='box-cox', standardize=True).fit(X[non_zer0])\n",
    "X[non_zer0] = scaler.transform(X[non_zer0])\n",
    "X_test[non_zer0] = scaler.transform(X_test[non_zer0])\n",
    "scaler = preprocessing.StandardScaler().fit(X[zero])\n",
    "X[zero] = scaler.transform(X[zero])\n",
    "X_test[zero] = scaler.transform(X_test[zero])\n",
    "\n",
    "meta_train, meta_test, meta_y_train, meta_y_test = model_selection.train_test_split(X,y,test_size=0.1,stratify=y)\n",
    "\n",
    "\n",
    "\n",
    "Xlr_train = meta_train\n",
    "Xlr_m_test = meta_test\n",
    "Xlr_test = X_test\n",
    "dummy = ['State','Type','Breed1','Breed2','Gender','Color1','Color2','Color3','Vaccinated','Dewormed','Sterilized']\n",
    "for d in dummy:\n",
    "    if(d in nAttributes):\n",
    "        \n",
    "        train = pd.get_dummies(Xlr_train[d],prefix=d)\n",
    "        test = pd.get_dummies(Xlr_test[d],prefix=d)\n",
    "        m_test = pd.get_dummies(Xlr_m_test[d],prefix=d)\n",
    "        result = set(list(train))\n",
    "        result.intersection_update(list(test))\n",
    "        result.intersection_update(list(m_test))\n",
    "        one_hottr = train[list(result)]\n",
    "        one_hot = test[list(result)]\n",
    "        one_hotm = m_test[list(result)]\n",
    "        Xlr_train = Xlr_train.drop(d,axis = 1)\n",
    "        # Join the encoded df\n",
    "        Xlr_train = Xlr_train.join(one_hottr)\n",
    "        \n",
    "        Xlr_test = Xlr_test.drop(d,axis = 1)\n",
    "        Xlr_test = Xlr_test.join(one_hot)\n",
    "        Xlr_m_test = Xlr_m_test.drop(d,axis=1)\n",
    "        Xlr_m_test = Xlr_m_test.join(one_hotm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classifiers\n",
    "\n",
    "The following classifiers were trained on the meta train data:\n",
    "\n",
    "+ Decision Tree classifier\n",
    "+ Random Forest classifier\n",
    "+ Logistic Regression\n",
    "+ Support Vector Machine classifier\n",
    "+ K-nearest Neighbor classifier\n",
    "+ Naive Bayes classifier\n",
    "+ XG Boost\n",
    "\n",
    "\n",
    "The predictions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
