{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final version of the classifiers and meta classifier script\n",
    "\n",
    "\n",
    "### Parameter settings\n",
    "\n",
    "First, parameters are set. The parameters are obtained from a crossvalidation performed for each seperate classifier. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Attributes from dataset that we use for classification\n",
    "\n",
    "nAttributes = ['Age', 'popularity resquer id', 'Breed1', 'img_pixels', 'PhotoAmt', 'Sterilized', 'description length',\n",
    "               'img_ave_contrast', 'Breed2', 'Quantity', 'Gender', 'img_metadata_sentiment2', 'beaut', 'MaturitySize',\n",
    "               'State',  'Color3', 'vaccin', 'abandon', 'Vaccinated', 'Fee', 'indoor', 'cute', 'great']\n",
    "\n",
    "\n",
    "max_depth = 8  #max depth of decision tree\n",
    "n_estimators = 10  # number of trees in random forest\n",
    "tol = 0.01  #tolerance in (gradient/line) search in Support Vector Machine classifier, Logistic Regression\n",
    "nn = 15  # number of neighbors for K-Nearest Neighbor classifier\n",
    "xgb_params = {  #parameters for XGBoost\n",
    "    'eval_metric': 'rmse',\n",
    "    'seed': 1337,\n",
    "    'verbosity': 0,\n",
    "}   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Some additional pre-processing\n",
    "\n",
    "Data is loaded and relevant attributes are obtained. A gaussian transform and normalisation transform is applied to the appropriate variables. Dataset is also converted with dummy variables for categorical attributes so that Logistic Regression can be performed. \n",
    "\n",
    "### Meta train/test set\n",
    "\n",
    "The train data is divided into a meta train and meta test set, to train and test the meta classifier respectively. 10% of the train data is used for meta testing, which helps select the best model for meta classifying.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lisa\\Anaconda3\\lib\\site-packages\\scipy\\stats\\morestats.py:902: RuntimeWarning: divide by zero encountered in log\n",
      "  llf -= N / 2.0 * np.log(np.sum((y - y_mean)**2. / N, axis=0))\n",
      "C:\\Users\\Lisa\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\data.py:625: DataConversionWarning: Data with input dtype int64, float64 were all converted to float64 by StandardScaler.\n",
      "  return self.partial_fit(X, y)\n",
      "C:\\Users\\Lisa\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:53: DataConversionWarning: Data with input dtype int64, float64 were all converted to float64 by StandardScaler.\n",
      "C:\\Users\\Lisa\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:54: DataConversionWarning: Data with input dtype int64, float64 were all converted to float64 by StandardScaler.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn import tree\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "import xgboost as xgb\n",
    "\n",
    "from sklearn import model_selection\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import PowerTransformer\n",
    "from sklearn import preprocessing\n",
    "\n",
    "\n",
    "\n",
    "data = pd.read_csv('Data/preprocessedTrain3.csv') #import data\n",
    "X = data.loc[:, data.columns != 'AdoptionSpeed'] #create X without labels\n",
    "X = X.fillna(0)\n",
    "X = X.drop('Description',axis=1) #drop non numerical values\n",
    "X = X.drop('PetID',axis=1) #\n",
    "X = X.drop('RescuerID',axis=1)\n",
    "X = X.drop('Unnamed: 0',axis=1)\n",
    "X = X.drop('Unnamed: 0.1',axis=1)\n",
    "X = X.drop('img_metadata_label',axis=1)\n",
    "X = X[nAttributes]\n",
    "y = data['AdoptionSpeed'] #label vector\n",
    "\n",
    "test = pd.read_csv('Data/preprocessedtest3.csv')\n",
    "\n",
    "\n",
    "X_test = test.drop('Description',axis=1) #drop non numerical values\n",
    "X_test = X_test.fillna(0)\n",
    "id = X_test['PetID']\n",
    "X_test = X_test.drop('PetID',axis=1) #\n",
    "X_test = X_test.drop('RescuerID',axis=1)\n",
    "X_test = X_test.drop('Unnamed: 0',axis=1)\n",
    "X_test = X_test.drop('Unnamed: 0.1',axis=1)\n",
    "\n",
    "X_test = X_test.drop('img_metadata_label',axis=1)\n",
    "X_test = X_test[nAttributes]\n",
    "\n",
    "\n",
    "non_zer0 = np.mean(X==0)==0\n",
    "zero = non_zer0[non_zer0.values==False].index\n",
    "non_zer0 = non_zer0[non_zer0.values==True].index\n",
    "\n",
    "scaler = preprocessing.PowerTransformer(method='box-cox', standardize=True).fit(X[non_zer0])\n",
    "X[non_zer0] = scaler.transform(X[non_zer0])\n",
    "X_test[non_zer0] = scaler.transform(X_test[non_zer0])\n",
    "scaler = preprocessing.StandardScaler().fit(X[zero])\n",
    "X[zero] = scaler.transform(X[zero])\n",
    "X_test[zero] = scaler.transform(X_test[zero])\n",
    "\n",
    "meta_train, meta_test, meta_y_train, meta_y_test = model_selection.train_test_split(X,y,test_size=0.1,stratify=y)\n",
    "\n",
    "\n",
    "\n",
    "Xlr_train = meta_train\n",
    "Xlr_m_test = meta_test\n",
    "Xlr_test = X_test\n",
    "dummy = ['State','Type','Breed1','Breed2','Gender','Color1','Color2','Color3','Vaccinated','Dewormed','Sterilized']\n",
    "for d in dummy:\n",
    "    if(d in nAttributes):\n",
    "        \n",
    "        train = pd.get_dummies(Xlr_train[d],prefix=d)\n",
    "        test = pd.get_dummies(Xlr_test[d],prefix=d)\n",
    "        m_test = pd.get_dummies(Xlr_m_test[d],prefix=d)\n",
    "        result = set(list(train))\n",
    "        result.intersection_update(list(test))\n",
    "        result.intersection_update(list(m_test))\n",
    "        one_hottr = train[list(result)]\n",
    "        one_hot = test[list(result)]\n",
    "        one_hotm = m_test[list(result)]\n",
    "        Xlr_train = Xlr_train.drop(d,axis = 1)\n",
    "        # Join the encoded df\n",
    "        Xlr_train = Xlr_train.join(one_hottr)\n",
    "        \n",
    "        Xlr_test = Xlr_test.drop(d,axis = 1)\n",
    "        Xlr_test = Xlr_test.join(one_hot)\n",
    "        Xlr_m_test = Xlr_m_test.drop(d,axis=1)\n",
    "        Xlr_m_test = Xlr_m_test.join(one_hotm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classifiers\n",
    "\n",
    "The following classifiers were trained on the meta train data:\n",
    "\n",
    "+ Decision Tree classifier\n",
    "+ Random Forest classifier\n",
    "+ Logistic Regression\n",
    "+ Support Vector Machine classifier\n",
    "+ K-nearest Neighbor classifier\n",
    "+ Naive Bayes classifier\n",
    "+ XG Boost\n",
    "\n",
    "\n",
    "The predictions from these classifiers on the train data, the meta test data and the actual test data are obtained. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lisa\\Anaconda3\\lib\\site-packages\\xgboost\\core.py:587: FutureWarning: Series.base is deprecated and will be removed in a future version\n",
      "  if getattr(data, 'base', None) is not None and \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\teval-rmse:1.80809\ttrain-rmse:1.80647\n",
      "Multiple eval metrics have been passed: 'train-rmse' will be used for early stopping.\n",
      "\n",
      "Will train until train-rmse hasn't improved in 3000 rounds.\n",
      "[3000]\teval-rmse:1.1454\ttrain-rmse:0.008655\n",
      "[6000]\teval-rmse:1.14561\ttrain-rmse:0.007215\n",
      "Stopping. Best iteration:\n",
      "[3305]\teval-rmse:1.14561\ttrain-rmse:0.007215\n",
      "\n"
     ]
    }
   ],
   "source": [
    "classifiers = 'DTC RF LOGREG KNN SVM GNB XGB'.split(sep=' ')\n",
    "predictions = np.zeros((len(X_test),len(classifiers)))\n",
    "mlp_train = np.zeros((len(meta_train),len(classifiers)))\n",
    "mlp_test = np.zeros((len(meta_test),len(classifiers)))\n",
    "\n",
    "dtc = tree.DecisionTreeClassifier(criterion='gini',max_depth=max_depth) #train decision tree\n",
    "dtc = dtc.fit(meta_train,meta_y_train)\n",
    "predictions[:,0] = dtc.predict(X_test)\n",
    "mlp_train[:,0] = dtc.predict(meta_train)\n",
    "mlp_test[:,0] = dtc.predict(meta_test)\n",
    "\n",
    "rf = RandomForestRegressor(n_estimators = n_estimators) #train random forest\n",
    "rf = rf.fit(meta_train, meta_y_train)\n",
    "predictions[:,1] = np.round(rf.predict(X_test),0)\n",
    "mlp_train[:,1] = np.round(rf.predict(meta_train),0)\n",
    "mlp_test[:,1] = np.round(rf.predict(meta_test),0)\n",
    "\n",
    "logreg = LogisticRegression(tol=tol,solver='liblinear',multi_class='auto')  # train logistic regressor\n",
    "logreg = logreg.fit(Xlr_train, meta_y_train)\n",
    "predictions[:,2] = logreg.predict(Xlr_test)\n",
    "mlp_train[:,2] = logreg.predict(Xlr_train)\n",
    "mlp_test[:,2] = logreg.predict(Xlr_m_test)\n",
    "\n",
    "knn = KNeighborsClassifier(nn)   # train KNN\n",
    "knn = knn.fit(meta_train, meta_y_train)\n",
    "predictions[:,3] = knn.predict(X_test)\n",
    "mlp_train[:,3] = knn.predict(meta_train)\n",
    "mlp_test[:,3] = knn.predict(meta_test)\n",
    "\n",
    "svm = SVC(tol=tol,gamma='auto')   # train SVM\n",
    "svm = svm.fit(meta_train, meta_y_train)\n",
    "predictions[:,4] = svm.predict(X_test)\n",
    "mlp_train[:,4] = svm.predict(meta_train)\n",
    "mlp_test[:,4] = svm.predict(meta_test)\n",
    "\n",
    "gnb = GaussianNB()   # train Naive Bayes\n",
    "gnb = gnb.fit(meta_train, meta_y_train)\n",
    "predictions[:,5] = gnb.predict(X_test)\n",
    "mlp_train[:,5] = dtc.predict(meta_train)\n",
    "mlp_test[:,5] = knn.predict(meta_test)\n",
    "\n",
    "d_train = xgb.DMatrix(data=meta_train, label=meta_y_train, feature_names=meta_train.columns)  # train XG boost\n",
    "d_val = xgb.DMatrix(data=meta_test,label=meta_y_test, feature_names=meta_test.columns)\n",
    "evallist = [(d_val, 'eval'), (d_train, 'train')]\n",
    "model = xgb.train(dtrain=d_train, num_boost_round=30000, evals=evallist, early_stopping_rounds=3000, verbose_eval=3000, params=xgb_params)\n",
    "predictions[:,6] = np.round(model.predict(xgb.DMatrix(X_test, feature_names=X_test.columns), ntree_limit=model.best_ntree_limit),0)\n",
    "mlp_train[:,6] = np.round(model.predict(xgb.DMatrix(meta_train, feature_names=meta_train.columns), ntree_limit=model.best_ntree_limit),0)\n",
    "mlp_test[:,6] = np.round(model.predict(xgb.DMatrix(meta_test, feature_names=meta_test.columns), ntree_limit=model.best_ntree_limit),0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classifier performances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train error for DTC is: 0.5428\n",
      "Test error for DTC is: 0.5807\n",
      "\n",
      "Train error for RF is: 0.2477\n",
      "Test error for RF is: 0.6827\n",
      "\n",
      "Train error for LOGREG is: 0.5924\n",
      "Test error for LOGREG is: 0.5987\n",
      "\n",
      "Train error for KNN is: 0.5409\n",
      "Test error for KNN is: 0.6227\n",
      "\n",
      "Train error for SVM is: 0.5056\n",
      "Test error for SVM is: 0.5773\n",
      "\n",
      "Train error for GNB is: 0.5428\n",
      "Test error for GNB is: 0.6227\n",
      "\n",
      "Train error for XGB is: 0.0001\n",
      "Test error for XGB is: 0.6760\n",
      "\n",
      "In total, 99.99% of the meta training set is classified correctly by at least one classifier\n",
      "In total, 79.87% of the meta test set is classified correctly by at least one classifier\n"
     ]
    }
   ],
   "source": [
    "correct = np.zeros((len(meta_train),len(classifiers)))\n",
    "correct_test = np.zeros((len(meta_test),len(classifiers)))\n",
    "for i in range(len(classifiers)):\n",
    "    err = 1-np.mean(mlp_train[:,i]==meta_y_train)\n",
    "    print('Train error for {} is: {:.4f}'.format(classifiers[i],err))\n",
    "    err = 1-np.mean(mlp_test[:,i]==meta_y_test)\n",
    "    print('Test error for {} is: {:.4f}'.format(classifiers[i],err))\n",
    "    print()\n",
    "    correct[:,i] = mlp_train[:,i] == meta_y_train\n",
    "    correct_test[:,i] = mlp_test[:,i] == meta_y_test\n",
    "    if(min(mlp_train[:,i])<0):\n",
    "        print(classifiers[i])\n",
    "        mlp_train[mlp_train[:,i]<0,i] = 0\n",
    "    if(np.any(np.isnan(mlp_train[:,i]))):\n",
    "        print(classifiers[i])\n",
    "        \n",
    "    if(max(mlp_train[:,i]>4)):\n",
    "        print(classifiers[i])\n",
    "        mlp_train[mlp_train[:,i]>4,i] = 4\n",
    "    if(np.all(np.isfinite(mlp_train[:,i]))==0):\n",
    "        print(classifiers[i])\n",
    "        \n",
    "correctdf = pd.DataFrame(correct)\n",
    "correct_testdf = pd.DataFrame(correct_test)\n",
    "print('In total, {:.2f}% of the meta training set is classified correctly by at least one classifier'.format(correctdf.max(axis=1).mean()*100))\n",
    "print('In total, {:.2f}% of the meta test set is classified correctly by at least one classifier'.format(correct_testdf.max(axis=1).mean()*100))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
